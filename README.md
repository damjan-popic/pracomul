# Pracomulâ€‘SLCÂ Analyzer

A  pipeline that transforms the Pracomul
**SLCâ€‘style dialogue corpus** (â‰ˆâ€¯108â€¯k tokens) into tidy data frames and
preâ€‘computed aggregate tablesâ€”ready for statistical analysis or quick
exploration in R, Python, Excel, or a notebook.

## 1â€¯Â Quick Start

```bash
# 1. (Optional) create a clean conda env
conda create -n slc python=3.10 spacy lexicalrichness pandas tqdm -c conda-forge
conda activate slc
python -m spacy download es_core_news_md  # Spanish model (â‰ˆâ€¯46â€¯MB)

# 2. Run the pipeline (from repo root)
python analyze_slc.py data/corpus.txt results
After ~10â€¯s on a modern laptop you will find 20â€‘odd CSVs in results/
(see Â§â€¯5).

2â€¯Â Project Rationale
In its raw form the Pracomul corpus is a single plainâ€‘text export:

php-template
Copy
Edit
<doc id="101" age_A="22" gender_A="F" â€¦ university_B="UniverzaÂ vÂ Ljubljani">
<text>
<A> Â¿quÃ© edad tienes? </A>
<B> Â¡Ah, sÃ­! â€¦ </B>
 â€¦
</text>
</doc>
To turn this into something a statistician can use we need to

recognise speaker turns (<A> / <B>),

normalise minor transcription artefacts,

attach all metadata (age, university, CEFR level, â€¦) to the
speaking person,

compute a battery of lexicalâ€‘diversity indices plus sentenceâ€‘level
diagnostics,

export perâ€‘token, perâ€‘turn, and aggregate tables.

All of that happens inside one selfâ€‘contained script:
analyze_slc.py.

3â€¯Â Dependencies
Library	Purpose	TestedÂ Version
PythonÂ â‰¥â€¯3.8	runtime	3.10.14
spaCy	tokenisationÂ Â· POSÂ Â· sentences	3.7.6
lexicalrichness	9 established LD indices	0.5.1
pandasÂ +Â NumPy	data wranglingÂ Â· aggregates	2.2.2 / 1.26.x
tqdm	progress bars	4.66

Everything is installable from condaâ€‘forge or PyPI.
No C/C++ toolchains required.

4â€¯Â Running the Script
text
Copy
Edit
Usage:  python analyze_slc.py <corpus.txt> <out_dir>
corpus.txt
the raw export (108â€¯k tokens, 65 DOC blocks).

out_dir
any folder; it will be created if absent.

On the first run spaCy will cache the Spanish model; subsequent runs are
pureâ€‘Python and need ~8â€“10â€¯seconds (including POS tagging).

5â€¯Â What You Get
pgsql
Copy
Edit
results/
â”œâ”€ turns.csv               # 1Â rowÂ = 1Â turn, A/B meta sideâ€‘byâ€‘side
â”œâ”€ turns_long.csv          # 1Â rowÂ = 1Â turn, speakerâ€‘centric columns
â”œâ”€ tokens.csv              # 1Â rowÂ = 1Â token  (â‰ˆ 108â€¯k rows)
â”œâ”€ tokens_with_meta.csv    # token rows + age / university / â€¦
â”œâ”€ aggregates_overall.csv  # corpusâ€‘wide means & sums
â”œâ”€ aggregates_by_doc.csv
â”œâ”€ aggregates_by_speaker.csv   # A vsÂ B
â”œâ”€ aggregates_by_university.csv
â”œâ”€ aggregates_by_nationality.csv
â”œâ”€ â€¦ and so on for age, gender, CEFR level, mother tongue
â””â”€ aggregates_by_country_combo.csv  # dyad: nationality_Aâ€‘nationality_B
5.1Â Perâ€‘Turn Columns (long format)
Variable	Meaning (speakerâ€‘centric)
token_count (N)	running words
type_countÂ (V)	distinct words
ttr	Vâ€¯/â€¯N
root_ttr	Guiraudâ€™s Vâ€¯/â€¯âˆšN
maas,Â hdd,Â â€¦	lengthâ€‘corrected LD indices
sent_len_mean	mean sentence length in tokens
laughter,Â â€¦	counts of pragmatic markers
age,Â gender,Â â€¦	speaker metadata (already aligned)

A complete dataâ€‘dictionary lives in docs/data_dictionary.md (or
see Â§â€¯7 below).

6â€¯Â Methodological Notes
Normalization
(RISAS) and other stage directions are stripped; arrow symbols are
deleted; informal*formal pairs keep the formal side.

Lexical Diversity
All indices come from lexicalrichness. Short turns (Nâ€¯â‰¤â€¯1) safely
returnÂ 0.0; root_ttr is computed manually if the attribute is absent
(older library versions).

Sentence Stats
spaCyâ€™s sentence segmenter is used. In conversational texts sentence
boundaries are admittedly fuzzy, but mean/median lengths still give a
coarse proxy for syntactic elaboration.

Long vsÂ Wide
turns_long.csv is the one you want for questions like
â€œWhatâ€™s the average MTLD of Slovenians, regardless of A/B role?â€

See /docs/methodology.pdf for a 6â€‘page writeâ€‘up citing GuiraudÂ 1954,
TweedieÂ &Â BaayenÂ 1998, MalvernÂ etâ€¯al.Â 2004, McCarthyÂ &Â JarvisÂ 2010, etc.

7â€¯Â Data Dictionary (short version)
File	Grain	Rows	Key columns
tokens_with_meta.csv	token	~108â€¯k	doc_id, speaker, turn_id, idx
turns_long.csv	turn	~4â€¯900	doc_id, speaker, turn_id
aggregates_by_*.csv	group	varies	grouping attr + numeric summaries

Each numeric summary has two flavours:

sumâ€‘like: token_count, laughter, question_marks, â€¦

meanâ€‘like: ttr, root_ttr, maas, hdd, mtld, sent_len_mean, â€¦

See /docs/data_dictionary.md for every column, unit,
and calculation formula.

8â€¯Â Reproducibility Tips
Freeze the environment

bash
Copy
Edit
conda list --explicit > env.lock.txt
Ship env.lock.txt if journal reviewers demand bitâ€‘forâ€‘bit
reproducibility; users can recreate with conda create --name slc --file env.lock.txt.

Version Control

Commit both analyze_slc.py and the raw corpus.txt SHAâ€‘256 hash.
Output CSVs can be generated on demand, so they stay out ofÂ Git.

Randomness

The pipeline has no stochastic components (even HDâ€‘Dâ€™s draws are
deterministic given the tokens), so reâ€‘runs are byteâ€‘identical.

9â€¯Â Troubleshooting
Symptom	Fix
Can't find model 'es_core_news_md'	Run python -m spacy download es_core_news_md
ModuleNotFoundError: lexicalrichness	pip install lexicalrichness or conda install -c conda-forge lexicalrichness
Memory (>â€¯2â€¯GB) on small VMs	Use the --disable ner switch already set in the script; RAM usage staysÂ <â€¯600â€¯MB
Aggregate CSV shows NaN in means	That group had only empty turns; perfectly fineâ€”filter or replace.

10â€¯Â Citation
If you use this script or the derived tables in a publication, please
cite both the corpus creators and this repository, e.g.:

Å½agar, Damjan. 2025. Pracomulâ€‘SLC Analyzer (VersionÂ 1.0).
GitHub. https://github.com/yourâ€‘handle/pracomulâ€‘slcâ€‘analyzer.

11â€¯Â License
The code is released under the MIT License.
The original Pracomul transcripts remain under their existing license
(nonâ€‘commercial research use).

Happy analysing! ðŸ”ðŸ—£ï¸

go
Copy
Edit

*(End of `README.md`)*